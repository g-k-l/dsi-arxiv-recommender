<oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.openarchives.org/OAI/2.0/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>On the convergence analysis of the optimized gradient method</dc:title>
 <dc:creator>Kim, Donghwan</dc:creator>
 <dc:creator>Fessler, Jeffrey A.</dc:creator>
 <dc:subject>Mathematics - Optimization and Control</dc:subject>
 <dc:description>  This paper considers the problem of unconstrained minimization of smooth
convex functions having Lipschitz continuous gradients with known Lipschitz
constant. We recently proposed an optimized gradient method (OGM) for this
problem and showed that it has a worst-case convergence bound for the cost
function decrease that is twice as small as that of Nesterov's fast gradient
method (FGM), yet has a similarly efficient practical implementation. Drori
showed recently that OGM has optimal complexity over the general class of
first-order methods. This optimality makes it important to study fully the
convergence properties of OGM. The previous worst-case convergence bound for
OGM was derived for only the last iterate of a secondary sequence. This paper
provides an analytic convergence bound for the primary sequence generated by
OGM. We then discuss additional convergence properties of OGM, including the
interesting fact that OGM has two types of worst-case functions: a piecewise
affine-quadratic function and a quadratic function. These results help complete
the theory of optimal first-order methods for smooth convex minimization.
</dc:description>
 <dc:date>2015-10-29</dc:date>
 <dc:date>2016-06-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1510.08573</dc:identifier>
 </oai_dc:dc>

