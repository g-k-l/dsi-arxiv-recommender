<oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.openarchives.org/OAI/2.0/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Binary Speaker Embedding</dc:title>
 <dc:creator>Li, Lantian</dc:creator>
 <dc:creator>Wang, Dong</dc:creator>
 <dc:creator>Xing, Chao</dc:creator>
 <dc:creator>Yu, Kaimin</dc:creator>
 <dc:creator>Zheng, Thomas Fang</dc:creator>
 <dc:subject>Computer Science - Sound</dc:subject>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:description>  The popular i-vector model represents speakers as low-dimensional continuous
vectors (i-vectors), and hence it is a way of continuous speaker embedding. In
this paper, we investigate binary speaker embedding, which transforms i-vectors
to binary vectors (codes) by a hash function. We start from locality sensitive
hashing (LSH), a simple binarization approach where binary codes are derived
from a set of random hash functions. A potential problem of LSH is that the
randomly sampled hash functions might be suboptimal. We therefore propose an
improved Hamming distance learning approach, where the hash function is learned
by a variable-sized block training that projects each dimension of the original
i-vectors to variable-sized binary codes independently. Our experiments show
that binary speaker embedding can deliver competitive or even better results on
both speaker verification and identification tasks, while the memory usage and
the computation cost are significantly reduced.
</dc:description>
 <dc:date>2015-10-20</dc:date>
 <dc:date>2016-03-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1510.05937</dc:identifier>
 </oai_dc:dc>

