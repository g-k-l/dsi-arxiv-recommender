<oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.openarchives.org/OAI/2.0/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>FairTest: Discovering Unwarranted Associations in Data-Driven
  Applications</dc:title>
 <dc:creator>Tram&#232;r, Florian</dc:creator>
 <dc:creator>Atlidakis, Vaggelis</dc:creator>
 <dc:creator>Geambasu, Roxana</dc:creator>
 <dc:creator>Hsu, Daniel</dc:creator>
 <dc:creator>Hubaux, Jean-Pierre</dc:creator>
 <dc:creator>Humbert, Mathias</dc:creator>
 <dc:creator>Juels, Ari</dc:creator>
 <dc:creator>Lin, Huang</dc:creator>
 <dc:subject>Computer Science - Computers and Society</dc:subject>
 <dc:description>  In a world where traditional notions of privacy are increasingly challenged
by the myriad companies that collect and analyze our data, it is important that
decision-making entities are held accountable for unfair treatments arising
from irresponsible data usage. Unfortunately, a lack of appropriate
methodologies and tools means that even identifying unfair or discriminatory
effects can be a challenge in practice. We introduce the unwarranted
associations (UA) framework, a principled methodology for the discovery of
unfair, discriminatory, or offensive user treatment in data-driven
applications. The UA framework unifies and rationalizes a number of prior
attempts at formalizing algorithmic fairness. It uniquely combines multiple
investigative primitives and fairness metrics with broad applicability,
granular exploration of unfair treatment in user subgroups, and incorporation
of natural notions of utility that may account for observed disparities. We
instantiate the UA framework in FairTest, the first comprehensive tool that
helps developers check data-driven applications for unfair user treatment. It
enables scalable and statistically rigorous investigation of associations
between application outcomes (such as prices or premiums) and sensitive user
attributes (such as race or gender). Furthermore, FairTest provides debugging
capabilities that let programmers rule out potential confounders for observed
unfair effects. We report on use of FairTest to investigate and in some cases
address disparate impact, offensive labeling, and uneven rates of algorithmic
error in four data-driven applications. As examples, our results reveal subtle
biases against older populations in the distribution of error in a predictive
health application and offensive racial labeling in an image tagger.
</dc:description>
 <dc:description>Comment: 27 pages, 12 figures</dc:description>
 <dc:date>2015-10-08</dc:date>
 <dc:date>2016-08-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1510.02377</dc:identifier>
 </oai_dc:dc>

