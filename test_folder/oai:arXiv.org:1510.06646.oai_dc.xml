<oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.openarchives.org/OAI/2.0/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A 'Gibbs-Newton' Technique for Enhanced Inference of Multivariate Polya
  Parameters and Topic Models</dc:title>
 <dc:creator>Khalifa, Osama</dc:creator>
 <dc:creator>Corne, David Wolfe</dc:creator>
 <dc:creator>Chantler, Mike</dc:creator>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:description>  Hyper-parameters play a major role in the learning and inference process of
latent Dirichlet allocation (LDA). In order to begin the LDA latent variables
learning process, these hyper-parameters values need to be pre-determined. We
propose an extension for LDA that we call 'Latent Dirichlet allocation Gibbs
Newton' (LDA-GN), which places non-informative priors over these
hyper-parameters and uses Gibbs sampling to learn appropriate values for them.
At the heart of LDA-GN is our proposed 'Gibbs-Newton' algorithm, which is a new
technique for learning the parameters of multivariate Polya distributions. We
report Gibbs-Newton performance results compared with two prominent existing
approaches to the latter task: Minka's fixed-point iteration method and the
Moments method. We then evaluate LDA-GN in two ways: (i) by comparing it with
standard LDA in terms of the ability of the resulting topic models to
generalize to unseen documents; (ii) by comparing it with standard LDA in its
performance on a binary classification task.
</dc:description>
 <dc:date>2015-10-22</dc:date>
 <dc:date>2016-02-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1510.06646</dc:identifier>
 </oai_dc:dc>

