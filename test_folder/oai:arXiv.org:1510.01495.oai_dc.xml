<oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.openarchives.org/OAI/2.0/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Quantifying Emergent Behavior of Autonomous Robots</dc:title>
 <dc:creator>Martius, Georg</dc:creator>
 <dc:creator>Olbrich, Eckehard</dc:creator>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:subject>Computer Science - Robotics</dc:subject>
 <dc:subject>Mathematics - Dynamical Systems</dc:subject>
 <dc:subject>68P30, 68T05, 68T40, 70E60, 93C10, 94A17</dc:subject>
 <dc:subject>H.1.1</dc:subject>
 <dc:subject>I.2.9</dc:subject>
 <dc:description>  Quantifying behaviors of robots which were generated autonomously from
task-independent objective functions is an important prerequisite for objective
comparisons of algorithms and movements of animals. The temporal sequence of
such a behavior can be considered as a time series and hence complexity
measures developed for time series are natural candidates for its
quantification. The predictive information and the excess entropy are such
complexity measures. They measure the amount of information the past contains
about the future and thus quantify the nonrandom structure in the temporal
sequence. However, when using these measures for systems with continuous states
one has to deal with the fact that their values will depend on the resolution
with which the systems states are observed. For deterministic systems both
measures will diverge with increasing resolution. We therefore propose a new
decomposition of the excess entropy in resolution dependent and resolution
independent parts and discuss how they depend on the dimensionality of the
dynamics, correlations and the noise level. For the practical estimation we
propose to use estimates based on the correlation integral instead of the
direct estimation of the mutual information using the algorithm by Kraskov et
al. (2004) which is based on next neighbor statistics because the latter allows
less control of the scale dependencies. Using our algorithm we are able to show
how autonomous learning generates behavior of increasing complexity with
increasing learning duration.
</dc:description>
 <dc:description>Comment: 24 pages, 10 figures, submitted Entropy Journal</dc:description>
 <dc:date>2015-10-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1510.01495</dc:identifier>
 <dc:identifier>Entropy 2015, 17(10), 7266-7297</dc:identifier>
 <dc:identifier>doi:10.3390/e17107266</dc:identifier>
 </oai_dc:dc>

