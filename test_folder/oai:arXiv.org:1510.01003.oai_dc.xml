<oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.openarchives.org/OAI/2.0/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Bayesian Estimation of Multidimensional Latent Variables and Its
  Asymptotic Accuracy</dc:title>
 <dc:creator>Yamazaki, Keisuke</dc:creator>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:description>  Hierarchical learning models, such as mixture models and Bayesian networks,
are widely employed for unsupervised learning tasks, such as clustering
analysis. They consist of observable and hidden variables, which represent the
given data and their hidden generation process, respectively. It has been
pointed out that conventional statistical analysis is not applicable to these
models, because redundancy of the latent variable produces singularities in the
parameter space. In recent years, a method based on algebraic geometry has
allowed us to analyze the accuracy of predicting observable variables when
using Bayesian estimation. However, how to analyze latent variables has not
been sufficiently studied, even though one of the main issues in unsupervised
learning is to determine how accurately the latent variable is estimated. A
previous study proposed a method that can be used when the range of the latent
variable is redundant compared with the model generating data. The present
paper extends that method to the situation in which the latent variables have
redundant dimensions. We formulate new error functions and derive their
asymptotic forms. Calculation of the error functions is demonstrated in
two-layered Bayesian networks.
</dc:description>
 <dc:date>2015-10-04</dc:date>
 <dc:date>2016-07-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1510.01003</dc:identifier>
 </oai_dc:dc>

