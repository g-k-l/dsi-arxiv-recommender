<oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.openarchives.org/OAI/2.0/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>When Are Nonconvex Problems Not Scary?</dc:title>
 <dc:creator>Sun, Ju</dc:creator>
 <dc:creator>Qu, Qing</dc:creator>
 <dc:creator>Wright, John</dc:creator>
 <dc:subject>Mathematics - Optimization and Control</dc:subject>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:description>  In this note, we focus on smooth nonconvex optimization problems that obey:
(1) all local minimizers are also global; and (2) around any saddle point or
local maximizer, the objective has a negative directional curvature. Concrete
applications such as dictionary learning, generalized phase retrieval, and
orthogonal tensor decomposition are known to induce such structures. We
describe a second-order trust-region algorithm that provably converges to a
global minimizer efficiently, without special initializations. Finally we
highlight alternatives, and open problems in this direction.
</dc:description>
 <dc:description>Comment: 6 pages, 3 figures. New examples on phase synchronization and
  community detection added; emphasis on all local minimizers being global
  added; exposition is polished. This is a concise expository article that
  avoids much technical rigor. We will make a separate submission with full
  technical details in future</dc:description>
 <dc:date>2015-10-20</dc:date>
 <dc:date>2016-04-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1510.06096</dc:identifier>
 </oai_dc:dc>

