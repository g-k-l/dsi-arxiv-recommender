<oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.openarchives.org/OAI/2.0/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>You-Do, I-Learn: Unsupervised Multi-User egocentric Approach Towards
  Video-Based Guidance</dc:title>
 <dc:creator>Damen, Dima</dc:creator>
 <dc:creator>Leelasawassuk, Teesid</dc:creator>
 <dc:creator>Mayol-Cuevas, Walterio</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  This paper presents an unsupervised approach towards automatically extracting
video-based guidance on object usage, from egocentric video and wearable gaze
tracking, collected from multiple users while performing tasks. The approach i)
discovers task relevant objects, ii) builds a model for each, iii)
distinguishes different ways in which each discovered object has been used and
iv) discovers the dependencies between object interactions. The work
investigates using appearance, position, motion and attention, and presents
results using each and a combination of relevant features. Moreover, an online
scalable approach is presented and is compared to offline results. The paper
proposes a method for selecting a suitable video guide to be displayed to a
novice user indicating how to use an object, purely triggered by the user's
gaze. The potential assistive mode can also recommend an object to be used next
based on the learnt sequence of object interactions. The approach was tested on
a variety of daily tasks such as initialising a printer, preparing a coffee and
setting up a gym machine.
</dc:description>
 <dc:date>2015-10-16</dc:date>
 <dc:date>2016-03-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1510.04862</dc:identifier>
 </oai_dc:dc>

