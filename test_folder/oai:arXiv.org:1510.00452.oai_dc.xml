<oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.openarchives.org/OAI/2.0/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Optimal Binary Classifier Aggregation for General Losses</dc:title>
 <dc:creator>Balsubramani, Akshay</dc:creator>
 <dc:creator>Freund, Yoav</dc:creator>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:description>  We address the problem of aggregating an ensemble of binary classifiers in a
semi-supervised setting. Recently, this problem was solved optimally using a
game-theoretic approach, but that analysis was specific to the 0-1 loss. In
this paper, we generalize the minimax optimal algorithm of the previous work to
a very general, novel class of loss functions, including but not limited to all
convex surrogates, while extending its performance and efficiency guarantees.
  The result is a family of parameter-free ensemble aggregation algorithms
which use labeled and unla- beled data; these are as efficient as linear
learning and prediction for convex risk minimization, but work without any
relaxations on many non-convex loss functions. The prediction algorithms take a
form familiar in decision theory, applying sigmoid functions to a generalized
notion of ensemble margin, but without the assumptions typically made in
margin-based learning.
</dc:description>
 <dc:description>Comment: NIPS 2015, "Learning Faster from Easy Data II" Workshop</dc:description>
 <dc:date>2015-10-01</dc:date>
 <dc:date>2016-02-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1510.00452</dc:identifier>
 </oai_dc:dc>

