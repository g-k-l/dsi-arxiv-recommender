<oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.openarchives.org/OAI/2.0/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Context-Aware Bandits</dc:title>
 <dc:creator>Li, Shuai</dc:creator>
 <dc:creator>Kar, Purushottam</dc:creator>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:subject>Computer Science - Artificial Intelligence</dc:subject>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:description>  We propose an efficient Context-Aware clustering of Bandits (CAB) algorithm,
which can capture collaborative effects. CAB can be easily deployed in a
real-world recommendation system, where multi-armed bandits have been shown to
perform well in particular with respect to the cold-start problem. CAB utilizes
a context-aware clustering augmented by exploration-exploitation strategies.
CAB dynamically clusters the users based on the content universe under
consideration. We give a theoretical analysis in the standard stochastic
multi-armed bandits setting. We show the efficiency of our approach on
production and real-world datasets, demonstrate the scalability, and, more
importantly, the significant increased prediction performance against several
state-of-the-art methods.
</dc:description>
 <dc:description>Comment: 18 pages, 4 figures. This version substantially extends the previous
  version by proposing a new, lighter version of the algorithm (CAB1), as well
  as giving regret bounds for both versions of the algorithm. The authors thank
  Alexandros Karatzoglou for his inputs on an earlier version of the paper</dc:description>
 <dc:date>2015-10-12</dc:date>
 <dc:date>2016-06-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1510.03164</dc:identifier>
 </oai_dc:dc>

