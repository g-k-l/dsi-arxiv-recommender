<oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.openarchives.org/OAI/2.0/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Optimization as Estimation with Gaussian Processes in Bandit Settings</dc:title>
 <dc:creator>Wang, Zi</dc:creator>
 <dc:creator>Zhou, Bolei</dc:creator>
 <dc:creator>Jegelka, Stefanie</dc:creator>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:description>  Recently, there has been rising interest in Bayesian optimization -- the
optimization of an unknown function with assumptions usually expressed by a
Gaussian Process (GP) prior. We study an optimization strategy that directly
uses an estimate of the argmax of the function. This strategy offers both
practical and theoretical advantages: no tradeoff parameter needs to be
selected, and, moreover, we establish close connections to the popular GP-UCB
and GP-PI strategies. Our approach can be understood as automatically and
adaptively trading off exploration and exploitation in GP-UCB and GP-PI. We
illustrate the effects of this adaptive tuning via bounds on the regret as well
as an extensive empirical evaluation on robotics and vision tasks,
demonstrating the robustness of this strategy for a range of performance
criteria.
</dc:description>
 <dc:date>2015-10-21</dc:date>
 <dc:date>2016-04-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1510.06423</dc:identifier>
 </oai_dc:dc>

