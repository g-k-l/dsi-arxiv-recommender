<oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.openarchives.org/OAI/2.0/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Asymptotic expansion of the risk of maximum likelihood estimator with
  respect to $\alpha$-divergence as a measure of the difficulty of specifying a
  parametric model --with detailed proof--</dc:title>
 <dc:creator>Sheena, Yo</dc:creator>
 <dc:subject>Mathematics - Statistics Theory</dc:subject>
 <dc:subject>60F99 (Primary), 62F12 (Secondary)</dc:subject>
 <dc:description>  For a given parametric probability model, we consider the risk of the maximum
likelihood estimator with respect to $\alpha$-divergence, which includes the
special cases of Kullback--Leibler divergence, the Hellinger distance and
$\chi^2$ divergence. The asymptotic expansion of the risk is given with respect
to sample sizes of up to order $n^{-2}$. Each term in the expansion is
expressed with the geometrical properties of the Riemannian manifold formed by
the parametric probability model. We attempt to measure the difficulty of
specifying a model through this expansion.
</dc:description>
 <dc:description>Comment: 84 pages, 3 figures</dc:description>
 <dc:date>2015-10-28</dc:date>
 <dc:date>2016-08-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1510.08226</dc:identifier>
 </oai_dc:dc>

