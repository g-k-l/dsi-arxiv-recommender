<oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.openarchives.org/OAI/2.0/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Similarity of symbol frequency distributions with heavy tails</dc:title>
 <dc:creator>Gerlach, Martin</dc:creator>
 <dc:creator>Font-Clos, Francesc</dc:creator>
 <dc:creator>Altmann, Eduardo G.</dc:creator>
 <dc:subject>Physics - Physics and Society</dc:subject>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:subject>Physics - Data Analysis, Statistics and Probability</dc:subject>
 <dc:description>  Quantifying the similarity between symbolic sequences is a traditional
problem in Information Theory which requires comparing the frequencies of
symbols in different sequences. In numerous modern applications, ranging from
DNA over music to texts, the distribution of symbol frequencies is
characterized by heavy-tailed distributions (e.g., Zipf's law). The large
number of low-frequency symbols in these distributions poses major difficulties
to the estimation of the similarity between sequences, e.g., they hinder an
accurate finite-size estimation of entropies. Here we show analytically how the
systematic (bias) and statistical (fluctuations) errors in these estimations
depend on the sample size~$N$ and on the exponent~$\gamma$ of the heavy-tailed
distribution. Our results are valid for the Shannon entropy $(\alpha=1)$, its
corresponding similarity measures (e.g., the Jensen-Shanon divergence), and
also for measures based on the generalized entropy of order $\alpha$. For small
$\alpha$'s, including $\alpha=1$, the errors decay slower than the $1/N$-decay
observed in short-tailed distributions. For $\alpha$ larger than a critical
value $\alpha^* = 1+1/\gamma \leq 2$, the $1/N$-decay is recovered. We show the
practical significance of our results by quantifying the evolution of the
English language over the last two centuries using a complete $\alpha$-spectrum
of measures. We find that frequent words change more slowly than less frequent
words and that $\alpha=2$ provides the most robust measure to quantify language
change.
</dc:description>
 <dc:description>Comment: 13 pages, 7 figures</dc:description>
 <dc:date>2015-10-01</dc:date>
 <dc:date>2016-04-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1510.00277</dc:identifier>
 <dc:identifier>Phys. Rev. X 6, 021009 (2016)</dc:identifier>
 <dc:identifier>doi:10.1103/PhysRevX.6.021009</dc:identifier>
 </oai_dc:dc>

